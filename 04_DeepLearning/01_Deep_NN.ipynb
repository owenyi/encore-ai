{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd6b8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a01d7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision # image processing에 특화된 라이브러리\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms # Data Augmentation에 특화된 라이브러리\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c463e066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.Tensor(2, 3) # 2행 3열의 텐서를 하나 생성\n",
    "X.shape # == X.size()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c5c611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb0611d",
   "metadata": {},
   "source": [
    "### tensor()인자값으로  \n",
    "    data : 값 지정, 초기화  \n",
    "    dtype  \n",
    "    requires_grad : tensor에 대해서 미분 적용할지 여부  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5448890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([4., 9.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([11., 21.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([3., 4.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(data=[2.0, 3.0], requires_grad=True)\n",
    "x\n",
    "\n",
    "y = x**2\n",
    "y\n",
    "\n",
    "pred = 2*y + 3\n",
    "pred\n",
    "\n",
    "target = torch.tensor([3.0, 4.0])\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4456fad2",
   "metadata": {},
   "source": [
    "### 위에서 나온 결과를 바탕으로 경사하강법을 이용해서 Loss를 줄여나가 보겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5510a7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.sum(torch.abs(pred - target))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07f2fe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0ba59ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8., 12.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-7755ea398175>:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  pred.grad\n"
     ]
    }
   ],
   "source": [
    "x.grad\n",
    "pred.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf92b43",
   "metadata": {},
   "source": [
    "### Create Tensor...Using NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15a040de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0965,  0.4992, -0.8314],\n",
       "        [ 0.3474, -0.7540,  0.0501],\n",
       "        [ 0.6691, -0.1490,  1.9303],\n",
       "        [ 0.8932, -1.4286, -0.7356],\n",
       "        [-1.5805, -0.1793, -0.2737],\n",
       "        [-0.2764, -1.3771, -1.0279],\n",
       "        [-1.7323,  0.4801, -0.1461],\n",
       "        [ 0.5173, -0.1094, -0.9523],\n",
       "        [-0.8775, -0.9870,  0.3024],\n",
       "        [ 0.8129,  0.4990, -1.3129]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4101,  0.0373],\n",
       "        [-0.7210,  0.8413],\n",
       "        [ 0.3187, -0.3910],\n",
       "        [-0.3194, -0.5916],\n",
       "        [ 0.5205,  0.4137],\n",
       "        [-0.7573, -0.4400],\n",
       "        [-1.8787, -0.5302],\n",
       "        [ 0.8019, -0.1632],\n",
       "        [ 0.6345, -1.4639],\n",
       "        [-0.3875,  0.2452]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "\n",
    "x\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efb6a798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2811, -0.0506, -0.2927],\n",
       "        [ 0.4379,  0.1187,  0.3818]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.4287, 0.1205], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(3, 2)\n",
    "linear.weight # == w\n",
    "linear.bias # == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc4366dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x000002634B374200>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 객체 출력, parameters()함수는 모델 안의 학습 주체인 w, b를 내포하고 있는 객체\n",
    "# parameters()함수는 w, b를 해킹한 함수이다\n",
    "linear.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d518169f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9968193769454956"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss function을 미리 정의\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# optimizer(하산하는 방법)를 미리 정의\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# 위에서 만든 모델에 값을 입력...결과로 예측 값이 나온다\n",
    "pred = linear(x)\n",
    "\n",
    "# 정답과 예측값을 이용해 위에서 미리 정의한 Loss를 계산...L(w)\n",
    "loss = loss_function(pred, y)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483a4af0",
   "metadata": {},
   "source": [
    "loss값이 나왔다는 것은  \n",
    "모델의 예측 값에 대한 잘잘못을 정량화했다는 것이다.  \n",
    "  \n",
    "이 값을 바탕으로 BackPropagation을 하면 w, b에 대한 미분값  \n",
    "즉 얼마만큼 보정해야 하는지가 나온다.  \n",
    "그 값을 이용해 다시 하강(기울기 수정)하기 때문에  \n",
    "2번째에는 Loss가 줄어들어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73c1beba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw tensor([[-0.4752, -0.2189, -0.5201],\n",
      "        [ 0.2220, -0.1781,  0.3561]])\n",
      "dL/db tensor([0.7092, 0.1107])\n"
     ]
    }
   ],
   "source": [
    "loss.backward() # loss값에 대한 w의 책임을 묻는다...미분을 적용, grad\n",
    "print(\"dL/dw\", linear.weight.grad)\n",
    "print(\"dL/db\", linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7caf28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss afer BackPropagation : 0.9842244982719421\n"
     ]
    }
   ],
   "source": [
    "optimizer.step() # 위에서 수정된 값으로 하강을 진행...학습\n",
    "\n",
    "# 반복효과\n",
    "pred = linear(x)\n",
    "loss = loss_function(pred, y)\n",
    "\n",
    "print(\"loss afer BackPropagation :\", loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
